{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from skimage import io, transform, exposure, color, util\n",
    "import os, itertools, sys\n",
    "from PIL import Image\n",
    "%pylab inline\n",
    "sys.setrecursionlimit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_dir = \"/home/dylan/IdeaProjects/distracted_drivers/train/\"\n",
    "data_dir =  \"/media/dylan/Science/Kaggle-Data/distracted_drivers/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_volume_shape = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_img_file_PIL(file_path, size=(32,32)):\n",
    "    img = Image.open(file_path).convert('L')\n",
    "    img.thumbnail(size, Image.NEAREST)\n",
    "    data = np.array(img)\n",
    "    shape = data.shape\n",
    "    append_top = int(ceil(max(0, size[0] - shape[0])/2.0))\n",
    "    append_bot = int(floor(max(0, size[0] - shape[0])/2.0))\n",
    "    data = util.pad(data, ((append_top, append_bot),\n",
    "                           (0,0)), mode='constant', constant_values=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img_file(file_path, rescale=0.01):\n",
    "    img = io.imread(file_path)\n",
    "    img= color.rgb2gray(img)\n",
    "    return transform.rescale(img, rescale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_gen_from_dir(directory, batch_size, num_categories, size=input_volume_shape):\n",
    "    result = {os.path.join(dp, f) : int(os.path.split(dp)[1]) for dp, dn, filenames in os.walk(data_dir) \n",
    "                  for f in filenames if os.path.splitext(f)[1] == '.jpg'}\n",
    "    # infinite loop\n",
    "    while True:\n",
    "        image_files = []\n",
    "        labels = []\n",
    "        # randomly choose batch size samples in result\n",
    "        for category in range(num_categories):\n",
    "            file_samples = np.random.choice([k for k, v in result.iteritems() if v == category], \n",
    "                             size=batch_size, replace=False)\n",
    "            for file_sample in file_samples:\n",
    "                image_files.append(read_img_file_PIL(file_sample, size=size))\n",
    "            labels.extend([v for v in itertools.repeat(category, batch_size)])\n",
    "\n",
    "        # end category loop\n",
    "        X = np.asarray(image_files, dtype=np.float32)\n",
    "        # -1 to 1 range\n",
    "        X = exposure.rescale_intensity(X, out_range=(-1,1))\n",
    "        y = np.asarray(labels, dtype=np.int32)\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another loader, augmentation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do 6 augmentations:\n",
    "\n",
    "\n",
    "    1.) Translation up to 10 pixels\n",
    "    2.) Rotation up to 15 degrees\n",
    "    3.) Zooming\n",
    "    4.) JPEG compression\n",
    "    5.) Sharpening\n",
    "    6.) Gamma correction\n",
    "\n",
    "\n",
    "We won't do flips since the dataset only contains images from the passenger seat. Perhaps we can revisit this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rotate, warp, AffineTransform\n",
    "from skimage import filters\n",
    "from scipy import ndimage, misc\n",
    "import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_translate(img):\n",
    "    shift_random = AffineTransform(translation=(randint(-10, 10), randint(-10, 10)))\n",
    "    min_value = 0 if min(img.ravel()) > 0 else min(img.ravel())\n",
    "    return np.float32(warp(img, shift_random, mode='constant', cval=min_value))\n",
    "\n",
    "def random_rotate(img):\n",
    "    min_value = 0 if min(img.ravel()) > 0 else min(img.ravel())\n",
    "    return np.float32(rotate(img, randint(-15, 15), mode='constant', cval=min_value))\n",
    "\n",
    "def random_zoom(img):\n",
    "    min_value = 0 if min(img.ravel()) > 0 else min(img.ravel())\n",
    "    scale_random = AffineTransform(scale=(uniform(0.9, 1.1), uniform(0.9, 1.1)))\n",
    "    return np.float32(warp(img, scale_random, mode='constant', cval=min_value))\n",
    "\n",
    "def random_compress(img):\n",
    "    max_v = np.ceil(img.max())\n",
    "    min_v = np.floor(img.min())\n",
    "    nd_im = exposure.rescale_intensity(img, out_range=(0, 1)).squeeze()\n",
    "    nd_im = np.ndarray.astype(nd_im * 255, np.uint8)\n",
    "    # nd_im = np.ndarray.astype(img * 255, np.uint8)\n",
    "    im = Image.fromarray(nd_im)\n",
    "    buf = StringIO.StringIO()\n",
    "    im.save(buf, \"JPEG\", quality=np.random.randint(95, 99))\n",
    "    buf.seek(0)\n",
    "    im2 = Image.open(buf)\n",
    "    x1 = exposure.rescale_intensity(np.ndarray.astype(np.array(im2), np.float32), out_range=(min_v, max_v))\n",
    "    return x1\n",
    "\n",
    "def random_sharpening(img):\n",
    "    blurred_f = ndimage.gaussian_filter(img, 0.5)\n",
    "    filter_blurred_f = ndimage.gaussian_filter(blurred_f, 1)\n",
    "    alpha = uniform(0.9, 1.2)\n",
    "    img = blurred_f + alpha * (blurred_f - filter_blurred_f)\n",
    "    return exposure.rescale_intensity(img, out_range=(-1 , 1))\n",
    "\n",
    "def random_gamma_correction(img):\n",
    "    max_v = np.ceil(img.max())\n",
    "    min_v = np.floor(img.min())\n",
    "    img = exposure.rescale_intensity(img, out_range=(0,1))\n",
    "    img = exposure.adjust_gamma(img, uniform(0.2, 0.8))\n",
    "    return exposure.rescale_intensity(img, out_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_aug(img):\n",
    "    choice = np.random.randint(0,6)\n",
    "    # choose from 4 different augmentations!\n",
    "    if choice == 0:\n",
    "        return random_translate(img)\n",
    "    elif choice == 1:\n",
    "        return random_rotate(img)\n",
    "    elif choice == 2:\n",
    "        return random_zoom(img)\n",
    "    elif choice == 3:\n",
    "        return random_compress(img)\n",
    "    elif choice == 4:\n",
    "        return random_sharpening(img)\n",
    "    else:\n",
    "        return random_gamma_correction(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_aug_batch(X, aug_algorithm):\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i] = aug_algorithm(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_aug_gen(gen, aug_algorithm):\n",
    "    for batchX, batchY in gen:\n",
    "        yield random_aug_batch(batchX, aug_algorithm), batchY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Generator with cached elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threaded_generator(generator, num_cached=50):\n",
    "    import Queue\n",
    "    queue = Queue.Queue(maxsize=num_cached)\n",
    "    sentinel = object()  # guaranteed unique reference\n",
    "\n",
    "    # define producer (putting items into queue)\n",
    "    def producer():\n",
    "        for item in generator:\n",
    "            queue.put(item)\n",
    "        queue.put(sentinel)\n",
    "\n",
    "    # start producer (in a background thread)\n",
    "    import threading\n",
    "    thread = threading.Thread(target=producer)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "    # run as consumer (read items from queue, in current thread)\n",
    "    item = queue.get()\n",
    "    while item is not sentinel:\n",
    "        yield item\n",
    "        queue.task_done()\n",
    "        item = queue.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, CuDNN 4004)\n"
     ]
    }
   ],
   "source": [
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.layers import DenseLayer, ReshapeLayer, Upscale2DLayer, Conv2DLayer, InputLayer, DropoutLayer, \\\n",
    "    MaxPool2DLayer, get_all_params, batch_norm\n",
    "import numpy as np\n",
    "from lasagne.nonlinearities import softmax, leaky_rectify\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo, objective\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from common import EarlyStopping, EndTrainingFromEarlyStopping\n",
    "from lasagne.objectives import categorical_crossentropy, aggregate\n",
    "import cPickle as pickle\n",
    "from sklearn import metrics\n",
    "import time, logging, logging.config, logging.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer, MaxPool2DDNNLayer\n",
    "    def conv_2_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DDNNLayer(top, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DDNNLayer(conv1, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DDNNLayer(conv2, (2, 2), 2)\n",
    "    \n",
    "    def conv_4_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DDNNLayer(top, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DDNNLayer(conv1, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        conv3 = batch_norm(Conv2DDNNLayer(conv2, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        conv4 = batch_norm(Conv2DDNNLayer(conv3, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DDNNLayer(conv4, (2, 2), 2)\n",
    "    \n",
    "    def conv_6_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DDNNLayer(top, num_filters, (3, 3), stride=1, pad=1,   nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DDNNLayer(conv1, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv3 = batch_norm(Conv2DDNNLayer(conv2, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv4 = batch_norm(Conv2DDNNLayer(conv3, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv5 = batch_norm(Conv2DDNNLayer(conv4, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv6 = batch_norm(Conv2DDNNLayer(conv5, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DLayer(conv6, (2, 2), 2)\n",
    "    \n",
    "except ImportError:\n",
    "    def conv_2_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DLayer(top, num_filters, (3, 3), stride=1, pad=1,   nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DLayer(conv1, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DLayer(conv2, (2, 2), 2)\n",
    "    \n",
    "    def conv_4_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DLayer(top, num_filters, (3, 3), stride=1, pad=0,   nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DLayer(conv1, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        conv3 = batch_norm(Conv2DLayer(conv2, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        conv4 = batch_norm(Conv2DLayer(conv3, num_filters, (3, 3), stride=1, pad=0, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DLayer(conv4, (2, 2), 2)\n",
    "    \n",
    "    def conv_6_layer_stack(top, num_filters):\n",
    "        conv1 = batch_norm(Conv2DLayer(top, num_filters, (3, 3), stride=1, pad=1,   nonlinearity=leaky_rectify))\n",
    "        conv2 = batch_norm(Conv2DLayer(conv1, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv3 = batch_norm(Conv2DLayer(conv2, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv4 = batch_norm(Conv2DLayer(conv3, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv5 = batch_norm(Conv2DLayer(conv4, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        conv6 = batch_norm(Conv2DLayer(conv5, num_filters, (3, 3), stride=1, pad=1, nonlinearity=leaky_rectify))\n",
    "        return MaxPool2DLayer(conv6, (2, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer = InputLayer((None, 1, input_volume_shape[0], input_volume_shape[1]))\n",
    "conv_stack_1 = conv_2_layer_stack(input_layer, 32)\n",
    "conv_stack_2 = conv_2_layer_stack(conv_stack_1, 64)\n",
    "conv_stack_3 = conv_4_layer_stack(conv_stack_2, 128)\n",
    "conv_stack_4 = conv_4_layer_stack(conv_stack_3, 256)\n",
    "dropout17 = DropoutLayer(conv_stack_4, p=0.5)\n",
    "dense18 = DenseLayer(dropout17, 2048, nonlinearity=leaky_rectify)\n",
    "dropout19 = DropoutLayer(dense18, p=0.5)\n",
    "dense20 = DenseLayer(dropout19, 2048, nonlinearity=leaky_rectify)\n",
    "softmax21 = DenseLayer(dense20, 10, nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Life Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "logging.config.fileConfig(\"logging-training.conf\")\n",
    "\n",
    "def regularization_objective(layers, lambda1=0., lambda2=0., *args, **kwargs):\n",
    "    # default loss\n",
    "    losses = objective(layers, *args, **kwargs)\n",
    "    # get layer weights except for the biases\n",
    "    weights = get_all_params(layers[-1], regularizable=True)\n",
    "    regularization_term = 0.0\n",
    "    # sum of abs weights for L1 regularization\n",
    "    if lambda1 != 0.0:\n",
    "        sum_abs_weights = sum([abs(w).sum() for w in weights])\n",
    "        regularization_term += (lambda1 * sum_abs_weights) \n",
    "    # sum of squares (sum(theta^2))\n",
    "    if lambda2 != 0.0:\n",
    "        sum_squared_weights = (1 / 2.0) * sum([(w ** 2).sum() for w in weights])\n",
    "        regularization_term += (lambda2 * sum_squared_weights)\n",
    "    # add weights to regular loss\n",
    "    losses += regularization_term\n",
    "    return losses\n",
    "\n",
    "def eval_regularization(net):\n",
    "    if net.objective_lambda1 == 0 and net.objective_lambda2 == 0:\n",
    "        return 0\n",
    "    # check the loss if the regularization term is not overpowering the loss\n",
    "    weights = get_all_params(net.layers_[-1], regularizable=True)\n",
    "    # sum of abs weights for L1 regularization\n",
    "    sum_abs_weights = sum([abs(w).sum() for w in weights])\n",
    "    # sum of squares (sum(theta^2))\n",
    "    sum_squared_weights = (1 / 2.0) * sum([(w ** 2).sum() for w in weights])\n",
    "    # add weights to regular loss\n",
    "    regularization_term = (net.objective_lambda1 * sum_abs_weights) \\\n",
    "                          + (net.objective_lambda2 * sum_squared_weights)\n",
    "    return regularization_term\n",
    "\n",
    "\n",
    "def print_regularization_term(net):\n",
    "    if net.objective_lambda1 > 0.0 or net.objective_lambda2 > 0.0:\n",
    "        regularization_term = eval_regularization(net)\n",
    "        print \"Regularization term: {}\".format(regularization_term.eval())\n",
    "\n",
    "def validation_set_loss(_net, _X, _y):\n",
    "    \"\"\"We need this to track the validation loss\"\"\"\n",
    "    _yb = _net.predict_proba(_X)\n",
    "    _y_pred = np.argmax(_yb, axis=1)\n",
    "    _acc = metrics.accuracy_score(_y, _y_pred)\n",
    "    loss = aggregate(categorical_crossentropy(_yb, _y))\n",
    "    loss += eval_regularization(_net)\n",
    "    return loss, _acc\n",
    "\n",
    "\n",
    "def store_model(model_file_name, net):\n",
    "    directory_name = os.path.dirname(model_file_name)\n",
    "    model_file_name = os.path.basename(model_file_name)\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "    # write model\n",
    "    output_model_file_name = os.path.join(directory_name, model_file_name)\n",
    "    start_write_time = time.time()\n",
    "    if os.path.isfile(output_model_file_name):\n",
    "        os.remove(output_model_file_name)\n",
    "    with open(output_model_file_name, 'wb') as experiment_model:\n",
    "        pickle.dump(net, experiment_model)\n",
    "    total_write_time = time.time() - start_write_time\n",
    "    m, s = divmod(total_write_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    logging.log(logging.INFO, \"Duration of saving to disk: %0d:%02d:%02d\", h, m, s)\n",
    "\n",
    "def write_validation_loss_and_store_best(validation_file_name, best_weights_file_name, \n",
    "                                         net, X_val, y_val, best_vloss, best_acc):\n",
    "    # write validation loss\n",
    "    start_validate_time = time.time()\n",
    "    vLoss, vAcc = validation_set_loss(net, X_val, y_val)\n",
    "    loss = vLoss.eval()\n",
    "    current_epoch = net.train_history_[-1]['epoch']\n",
    "    with open(validation_file_name, 'a') as validation_file:\n",
    "        validation_file.write(\"{}, {}, {}\\n\".format(current_epoch, loss, vAcc))\n",
    "\n",
    "    total_validate_time = time.time() - start_validate_time\n",
    "    m, s = divmod(total_validate_time, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    logging.log(logging.INFO, \"Duration of validation: %0d:%02d:%02d\", h, m, s)\n",
    "    \n",
    "    # store best weights here\n",
    "    if loss < best_vloss:\n",
    "        start_bw_time = time.time()\n",
    "        best_vloss = loss\n",
    "        best_acc = vAcc\n",
    "        with open(best_weights_file_name, 'wb') as best_model_file:\n",
    "            pickle.dump(net.get_all_params_values(), best_model_file, -1)\n",
    "            \n",
    "    return best_vloss, best_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dir =  \"/media/dylan/Science/Kaggle-Data/distracted_drivers/val/\"\n",
    "X_val, y_val = image_gen_from_dir(val_dir, 40, 10, size=input_volume_shape).next()\n",
    "X_val = X_val.reshape(-1, 1, input_volume_shape[0], input_volume_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda1 = 0.0\n",
    "lambda2 = 5e-4\n",
    "\n",
    "net = NeuralNet(\n",
    "    layers=softmax21,\n",
    "    max_epochs=1,\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.0001,\n",
    "    update_momentum = 0.9,\n",
    "    # update=adam,\n",
    "    on_epoch_finished=[\n",
    "        EarlyStopping(patience=2000)\n",
    "    ],\n",
    "    on_training_finished=[\n",
    "        EndTrainingFromEarlyStopping()\n",
    "    ],\n",
    "    objective=regularization_objective,\n",
    "    objective_lambda2=lambda2,\n",
    "    objective_lambda1=lambda1,\n",
    "    batch_iterator_train=BatchIterator(batch_size=100),\n",
    "    train_split=TrainSplit(\n",
    "        eval_size=0.25),\n",
    "    # train_split=TrainSplit(eval_size=0.0),\n",
    "    verbose=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = PrintLayerInfo()\n",
    "net.initialize()\n",
    "# p(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 8964778 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "name               size          total    cap.Y    cap.X    cov.Y    cov.X    filter Y    filter X    field Y    field X\n",
      "-----------------  ----------  -------  -------  -------  -------  -------  ----------  ----------  ---------  ---------\n",
      "InputLayer         1x128x128     16384   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     32x128x128   524288   100.00   100.00     2.34     2.34           3           3          3          3\n",
      "BatchNormLayer     32x128x128   524288   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  32x128x128   524288   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     32x128x128   524288   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     32x128x128   524288   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  32x128x128   524288   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "MaxPool2DDNNLayer  32x64x64     131072   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  64x64x64     262144   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "MaxPool2DDNNLayer  64x32x32      65536   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     128x30x30    115200   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     128x30x30    115200   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  128x30x30    115200   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     128x28x28    100352   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     128x28x28    100352   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  128x28x28    100352   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     128x26x26     86528   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     128x26x26     86528   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  128x26x26     86528   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     128x24x24     73728   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     128x24x24     73728   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  128x24x24     73728   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "MaxPool2DDNNLayer  128x12x12     18432   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     256x10x10     25600   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     256x10x10     25600   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  256x10x10     25600   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     256x8x8       16384   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     256x8x8       16384   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  256x8x8       16384   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     256x6x6        9216   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     256x6x6        9216   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  256x6x6        9216   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "Conv2DDNNLayer     256x4x4        4096   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "BatchNormLayer     256x4x4        4096   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "NonlinearityLayer  256x4x4        4096   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "MaxPool2DDNNLayer  256x2x2        1024   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "DropoutLayer       256x2x2        1024   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "DenseLayer         2048           2048   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "DropoutLayer       2048           2048   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "DenseLayer         2048           2048   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "DenseLayer         10               10   100.00   100.00   100.00   100.00         128         128        128        128\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m3.23474\u001b[0m       \u001b[32m2.47064\u001b[0m      1.30927      0.06667  0.92s\n",
      "      2       \u001b[36m3.13003\u001b[0m       2.47110      1.26665      0.03333  0.90s\n",
      "      3       \u001b[36m2.99001\u001b[0m       \u001b[32m2.46972\u001b[0m      1.21066      0.06667  1.01s\n",
      "      4       3.25569       \u001b[32m2.46959\u001b[0m      1.31831      0.10000  0.88s\n",
      "      5       3.13186       2.47047      1.26772      0.06667  0.90s\n",
      "      6       3.10504       2.46985      1.25718      0.10000  0.91s\n",
      "      7       3.49868       2.48008      1.41071      0.06667  0.90s\n",
      "      8       3.24035       2.47678      1.30829      0.10000  0.90s\n",
      "      9       3.00858       2.48594      1.21024      0.10000  0.95s\n",
      "     10       3.07501       2.48129      1.23928      0.06667  0.90s\n",
      "     11       3.01663       2.47993      1.21642      0.13333  0.92s\n",
      "     12       3.03845       \u001b[32m2.45505\u001b[0m      1.23763      0.06667  0.92s\n",
      "     13       \u001b[36m2.91237\u001b[0m       2.50508      1.16259      0.10000  0.91s\n",
      "     14       \u001b[36m2.85795\u001b[0m       2.46097      1.16131      0.16667  0.90s\n",
      "     15       \u001b[36m2.80923\u001b[0m       2.47287      1.13602      0.13333  0.91s\n",
      "     16       3.01027       2.48820      1.20982      0.10000  0.90s\n",
      "     17       \u001b[36m2.80844\u001b[0m       2.50207      1.12245      0.10000  0.90s\n",
      "     18       2.86454       \u001b[32m2.44450\u001b[0m      1.17183      0.10000  0.89s\n",
      "     19       \u001b[36m2.79950\u001b[0m       2.51300      1.11401      0.03333  0.90s\n",
      "     20       3.02547       2.56349      1.18022      0.03333  0.91s\n",
      "     21       2.82010       2.53362      1.11307      0.10000  0.89s\n",
      "     22       2.82993       \u001b[32m2.42642\u001b[0m      1.16630      0.16667  0.89s\n",
      "     23       3.12718       2.56695      1.21825      0.10000  0.90s\n",
      "     24       \u001b[36m2.77314\u001b[0m       2.54414      1.09001      0.10000  0.91s\n",
      "     25       2.95101       2.60550      1.13261      0.10000  0.90s\n",
      "     26       2.85654       2.49399      1.14537      0.10000  0.97s\n",
      "     27       2.84319       \u001b[32m2.41811\u001b[0m      1.17579      0.16667  0.95s\n",
      "     28       \u001b[36m2.72599\u001b[0m       2.58565      1.05428      0.10000  0.89s\n",
      "     29       3.06946       2.59261      1.18393      0.10000  0.90s\n",
      "     30       2.81219       2.51542      1.11798      0.10000  0.91s\n",
      "     31       3.06138       2.54847      1.20126      0.13333  0.91s\n",
      "     32       2.82345       2.49851      1.13006      0.16667  0.90s\n",
      "     33       2.90232       2.50254      1.15975      0.10000  0.89s\n",
      "     34       2.75007       2.53666      1.08413      0.06667  0.90s\n",
      "     35       2.95241       2.62377      1.12525      0.06667  0.91s\n",
      "     36       3.04474       2.60348      1.16949      0.00000  0.92s\n",
      "     37       2.92750       2.50788      1.16732      0.10000  0.90s\n",
      "     38       \u001b[36m2.53108\u001b[0m       2.58230      0.98017      0.13333  0.90s\n",
      "     39       3.09970       2.52990      1.22523      0.10000  0.90s\n",
      "     40       2.67083       2.54307      1.05024      0.10000  0.90s\n",
      "     41       2.88905       2.44300      1.18258      0.13333  0.92s\n",
      "     42       3.00076       2.63735      1.13779      0.03333  0.91s\n",
      "     43       2.84186       2.66385      1.06683      0.10000  0.91s\n",
      "     44       2.83282       2.52885      1.12020      0.03333  0.90s\n",
      "     45       2.93888       2.58979      1.13479      0.13333  0.91s\n",
      "     46       2.75568       2.49318      1.10529      0.20000  0.89s\n",
      "     47       2.78819       2.60775      1.06919      0.10000  0.90s\n",
      "     48       3.00252       2.60190      1.15397      0.00000  0.89s\n",
      "     49       2.85724       2.55592      1.11789      0.13333  0.89s\n",
      "     50       2.94593       2.53239      1.16330      0.10000  0.89s\n",
      "     51       3.03594       2.52270      1.20345      0.10000  0.90s\n",
      "     52       2.81859       2.55127      1.10478      0.16667  0.89s\n",
      "     53       2.77743       2.54951      1.08940      0.10000  0.91s\n",
      "     54       2.94093       2.44697      1.20186      0.23333  0.91s\n",
      "     55       2.95946       2.48883      1.18910      0.03333  0.91s\n",
      "     56       2.76457       2.49127      1.10970      0.06667  0.91s\n",
      "     57       2.81538       2.52222      1.11623      0.06667  0.90s\n",
      "     58       2.63187       2.62455      1.00279      0.10000  0.90s\n",
      "     59       2.86669       2.62714      1.09118      0.10000  0.90s\n",
      "     60       2.80323       2.58619      1.08392      0.16667  0.91s\n",
      "     61       2.78928       2.59084      1.07659      0.10000  0.90s\n",
      "     62       2.78297       2.49973      1.11331      0.10000  0.90s\n",
      "     63       2.99439       2.51418      1.19100      0.10000  0.91s\n",
      "     64       2.85013       2.54657      1.11920      0.10000  0.90s\n",
      "     65       2.75358       2.55932      1.07590      0.13333  0.90s\n",
      "     66       2.81624       2.56078      1.09976      0.10000  0.91s\n",
      "     67       2.81874       2.50096      1.12706      0.10000  0.91s\n",
      "     68       2.84398       2.63137      1.08080      0.20000  0.91s\n",
      "     69       2.84880       2.51919      1.13084      0.06667  0.91s\n",
      "     70       2.93559       2.53103      1.15984      0.06667  0.90s\n",
      "     71       2.70180       2.45847      1.09898      0.06667  0.90s\n"
     ]
    }
   ],
   "source": [
    "image_gen = image_gen_from_dir(data_dir, 10, 10, size=input_volume_shape)\n",
    "gen = random_aug_gen(image_gen, random_aug)\n",
    "threaded_gen = threaded_generator(gen, num_cached=100)\n",
    "\n",
    "ops_every = 100\n",
    "dir_name = 'net.vgg.large.l2.5e4'\n",
    "validation_file_name = \"{}/vloss-{}.txt\".format(dir_name, dir_name)\n",
    "model_file_name = \"{}/{}.pickle\".format(dir_name, dir_name)\n",
    "best_weights_file_name = \"{}/bw-{}.weights\".format(dir_name, dir_name)\n",
    "best_acc = 0.0\n",
    "best_vloss = np.inf\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    for step, (inputs, targets) in enumerate(threaded_gen):\n",
    "        shape = inputs.shape\n",
    "        net.fit(inputs.reshape(shape[0],1, shape[1], shape[2]), targets)\n",
    "        if (step + 1) % ops_every == 0:\n",
    "            print_regularization_term(net)\n",
    "            store_model(model_file_name, net)\n",
    "            # center validation\n",
    "            best_vloss, best_acc = write_validation_loss_and_store_best(\n",
    "                validation_file_name, best_weights_file_name, net, X_val, y_val, best_vloss, best_acc)\n",
    "            \n",
    "except StopIteration:\n",
    "    # terminate if already early stopping\n",
    "    with open(\"net.vgg.large.pickle\", 'wb') as writer:\n",
    "        pickle.dump(net, writer)\n",
    "    total_time = time.time() - start_time \n",
    "    print(\"Training successful by early stopping. Elapsed: {}\".format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
